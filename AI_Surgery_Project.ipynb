{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d044db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statistics import mean\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import random \n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea0b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SurgicalEnvironment:\n",
    "    def __init__(self):\n",
    "        # Define the size of the surgical environment\n",
    "        self.dimension = 10\n",
    "\n",
    "        # Initialize the surgical grid with default values\n",
    "        self.surgical_grid = -np.ones((self.dimension + 1, self.dimension + 1), dtype=np.int32)\n",
    "\n",
    "        # Initial scalpel position\n",
    "        self.scalpel_position = (4, 3)\n",
    "        self.original_scalpel_coords = (4, 3)\n",
    "\n",
    "        # Define object, target, bladder, avoid, and peritoneum positions and their values\n",
    "        self.object_positions = [(4, 4), (4, 5), (4, 6), (5, 4), (5, 5), (5, 6)] #prostate\n",
    "        self.object_value = 128\n",
    "\n",
    "        self.target_positions = [(3, 4), (3, 6), (4, 3), (4, 7), (6, 5), (3, 5)] #the points to cut\n",
    "        self.target_value = 16\n",
    "\n",
    "        self.bladder_positions = [(1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (2, 5)] #the bladder, it must be avoided more then other points\n",
    "        self.bladder_value = 32\n",
    "\n",
    "        self.avoid_positions = [(2, 4), (2, 3), (2, 6), (2, 7), (7, 5), (8, 5), (9, 5), (4, 2), (4, 8)] #point to be avoided such as nerves and bladder conducts\n",
    "        self.avoid_value = 64\n",
    "\n",
    "        self.peritoneum_value = 255\n",
    "        self.scalpel_value = 8\n",
    "\n",
    "        self.path_history = []\n",
    "        self.actions_taken = []\n",
    "\n",
    "        # Define the action space\n",
    "        self.action_space = [0, 1, 2, 3]\n",
    "        self.actions_dict = {0: 'up', 1: 'right', 2: 'down', 3: 'left'}\n",
    "        self.num_actions = len(self.action_space)\n",
    "\n",
    "    def create_environment(self):\n",
    "        # Set boundary values to 0\n",
    "        self.surgical_grid[0] = self.surgical_grid[-1] = self.surgical_grid[:, 0] = self.surgical_grid[:, -1] = 0\n",
    "\n",
    "        # Set values for objects, targets, bladder, and positions to avoid\n",
    "        for position in self.object_positions:\n",
    "            self.surgical_grid[position] = self.object_value\n",
    "\n",
    "        for position in self.target_positions:\n",
    "            self.surgical_grid[position] = self.target_value\n",
    "\n",
    "        for position in self.avoid_positions:\n",
    "            self.surgical_grid[position] = self.avoid_value\n",
    "\n",
    "        for position in self.bladder_positions:\n",
    "            self.surgical_grid[position] = self.bladder_value\n",
    "\n",
    "        # Set remaining undefined points to peritoneum value\n",
    "        self.surgical_grid[self.surgical_grid == -1] = self.peritoneum_value\n",
    "\n",
    "        return self.surgical_grid.copy()\n",
    "\n",
    "    def display_environment(self):\n",
    "        # Display the surgical environment\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(self.surgical_grid)\n",
    "        plt.show()\n",
    "\n",
    "    def perform_action(self, action):\n",
    "        reward = 0\n",
    "        done = False\n",
    "        self.path_history.append(self.scalpel_position)\n",
    "\n",
    "        # Define movement vectors for actions\n",
    "        dx, dy = [(0, -1), (1, 0), (0, 1), (-1, 0)][action]\n",
    "        new_x, new_y = self.scalpel_position[0] + dx, self.scalpel_position[1] + dy\n",
    "\n",
    "        cell_value = self.surgical_grid[new_x, new_y]\n",
    "\n",
    "        if cell_value == 0:\n",
    "            reward = -5\n",
    "            done = True\n",
    "        elif cell_value == self.object_value or cell_value == self.bladder_value:\n",
    "            reward = -5\n",
    "            done = True\n",
    "        elif cell_value == self.target_value:\n",
    "            reward = +10\n",
    "        elif cell_value == self.avoid_value:\n",
    "            reward = -3\n",
    "        elif cell_value == self.peritoneum_value:\n",
    "            reward = -1\n",
    "        elif cell_value == self.scalpel_value:\n",
    "            reward = -3\n",
    "\n",
    "        self.surgical_grid[self.scalpel_position] = self.scalpel_value\n",
    "        self.scalpel_position = (new_x, new_y)\n",
    "\n",
    "        if not np.any(self.surgical_grid == self.target_value):\n",
    "            done = True\n",
    "\n",
    "        self.actions_taken.append(self.actions_dict[action])\n",
    "\n",
    "        return self.surgical_grid.copy(), reward, done, {'path_history': self.path_history, 'actions_taken': self.actions_taken}\n",
    "\n",
    "    def reset_environment(self):\n",
    "        # Reset the surgical environment\n",
    "        self.surgical_grid = self.create_environment()\n",
    "        self.path_history = []\n",
    "        self.actions_taken = []\n",
    "        self.scalpel_position = self.original_scalpel_coords\n",
    "        self.surgical_grid[self.scalpel_position] = self.scalpel_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64351336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "surgical_environment = SurgicalEnvironment()\n",
    "surgical_grid = surgical_environment.create_environment()\n",
    "surgical_environment.display_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "850ae722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 9, 9, 16)          160       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1296)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                41504     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,796\n",
      "Trainable params: 41,796\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_actions = surgical_environment.num_actions\n",
    "input_shape = (surgical_environment.dimension + 1, surgical_environment.dimension + 1, 1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_actions, activation='linear'))\n",
    "model.compile(optimizer=RMSprop(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f2872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_state(state):\n",
    "    state = np.expand_dims(state, axis=0)  # Add batch dimension\n",
    "    state = np.expand_dims(state, axis=-1)  # Add channel dimension\n",
    "    state = state.astype('float32') / 255.0  # Normalize state values between 0 and 1\n",
    "    return state\n",
    "\n",
    "def experience_replay(training_model, replay_memory, minibatch_size, gamma=0.7):\n",
    "    inputs = np.zeros((minibatch_size, *input_shape))\n",
    "    targets = np.zeros((minibatch_size, num_actions))\n",
    "\n",
    "    minibatch = np.random.choice(replay_memory, minibatch_size, replace=True)\n",
    "\n",
    "    state_list = np.array(list(map(lambda x: x['s'], minibatch)))\n",
    "    action_list = np.array(list(map(lambda x: x['a'], minibatch)))\n",
    "    reward_list = np.array(list(map(lambda x: x['r'], minibatch)))\n",
    "    s_prime_list = np.array(list(map(lambda x: x['s_prime'], minibatch)))\n",
    "    done_list = np.array(list(map(lambda x: x['done'], minibatch)))\n",
    "\n",
    "    for i, (s, a, r, sprime, done) in enumerate(zip(state_list, action_list, reward_list, s_prime_list, done_list)):\n",
    "        inputs[i] = s\n",
    "        targets[i] = training_model.predict(s)\n",
    "        Q_sa = np.max(training_model.predict(sprime))\n",
    "        if not done:\n",
    "            targets[i, a] = r + gamma * Q_sa\n",
    "        else:\n",
    "            targets[i, a] = r\n",
    "\n",
    "    training_model.fit(inputs, targets, epochs=10, verbose=0)\n",
    "    return training_model\n",
    "\n",
    "def animate_frame(i):\n",
    "    im.set_array(frames[i])\n",
    "    return [im]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a741598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(num_episodes, minibatch_size, memory_max_size, initial_epsilon, epsilon_decay, gamma, learning_rate):\n",
    "    min_epsilon = 0.01\n",
    "    reward_sums = []\n",
    "    replay_memory = []\n",
    "\n",
    "    # Training loop\n",
    "    for episode in range(num_episodes):\n",
    "        surgical_environment.reset_environment()\n",
    "        done = False\n",
    "        reward_sum = 0\n",
    "\n",
    "        while not done:\n",
    "            state = preprocess_state(surgical_environment.surgical_grid)\n",
    "\n",
    "            if np.random.rand() < initial_epsilon:\n",
    "                action = random.choice(surgical_environment.action_space)\n",
    "            else:\n",
    "                q_values = model.predict(state)\n",
    "                action = np.argmax(q_values)\n",
    "\n",
    "            next_state, reward, done, _ = surgical_environment.perform_action(action)\n",
    "            reward_sum += reward\n",
    "\n",
    "            replay_memory.append({'s': state, 'a': action, 'r': reward, 's_prime': preprocess_state(next_state), 'done': done})\n",
    "\n",
    "            if len(replay_memory) > memory_max_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            if len(replay_memory) >= minibatch_size:\n",
    "                minibatch = random.sample(replay_memory, minibatch_size)\n",
    "                inputs = np.zeros((minibatch_size, *input_shape))\n",
    "                targets = np.zeros((minibatch_size, num_actions))\n",
    "\n",
    "                for i, sample in enumerate(minibatch):\n",
    "                    s, a, r, sprime, d = sample['s'], sample['a'], sample['r'], sample['s_prime'], sample['done']\n",
    "                    inputs[i] = s\n",
    "                    targets[i] = model.predict(s)\n",
    "                    if not d:\n",
    "                        targets[i, a] = r + gamma * np.max(model.predict(sprime))\n",
    "                    else:\n",
    "                        targets[i, a] = r\n",
    "\n",
    "                model.fit(inputs, targets, epochs=1, verbose=0)\n",
    "\n",
    "        initial_epsilon *= epsilon_decay\n",
    "        initial_epsilon = max(initial_epsilon, min_epsilon)\n",
    "        reward_sums.append(reward_sum)\n",
    "        plt.plot(reward_sums, color='blue', linewidth=1)  # plot of the rewards\n",
    "        if episode >= 3:  \n",
    "            x = np.linspace(0, len(reward_sums), len(reward_sums), dtype=np.int32)\n",
    "            z = np.polyfit(x, reward_sums, 3)\n",
    "            p = np.poly1d(z)\n",
    "            plt.plot(x, p(x), linewidth=2, color='orange')\n",
    "\n",
    "        plt.title(f'Episode {episode+1},  \\u03B5= {round(initial_epsilon, 3)},  Avg reward={round(mean(reward_sums), 3)}')\n",
    "\n",
    "        if episode != num_episodes - 1:  # until the training is not finished\n",
    "            plt.draw()\n",
    "            plt.pause(1.0)\n",
    "            plt.clf()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}/{num_episodes}, Total Reward: {reward_sum}\")\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save('./improved_agent_' + str(num_episodes) + '_' + str(minibatch_size) + '_' + str(memory_max_size) + '_' + str(initial_epsilon) + '_' + str(epsilon_decay) + '_' + str(gamma) + '_' + str(learning_rate) + '.h5') \n",
    "    \n",
    "    ## Testing\n",
    "\n",
    "    trained_model = load_model('./improved_agent_' + str(num_episodes) + '_' + str(minibatch_size) + '_' + str(memory_max_size) + '_' + str(initial_epsilon) + '_' + str(epsilon_decay) + '_' + str(gamma) + '_' + str(learning_rate) + '.h5')  # loading the trained model\n",
    "\n",
    "    test_environment = SurgicalEnvironment()\n",
    "    test_environment.reset_environment()\n",
    "\n",
    "    finished = False\n",
    "    total_reward = 0\n",
    "    frames = [test_environment.surgical_grid.copy()]  # create frames for animation\n",
    "\n",
    "    while not finished:\n",
    "        state = preprocess_state(test_environment.surgical_grid)\n",
    "        q_values = trained_model.predict(state)\n",
    "        action = np.argmax(q_values)\n",
    "        _, reward, finished, info = test_environment.perform_action(action)\n",
    "        print('Path:', info['path'])\n",
    "        print('Actions:', info['actions_taken'])\n",
    "        print()\n",
    "        total_reward += reward\n",
    "\n",
    "        frames.append(test_environment.surgical_grid.copy())  # store the frame\n",
    "\n",
    "    print('Total reward:', total_reward)\n",
    "    \n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8673589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid values to search through\n",
    "num_episodes_values = [5000]\n",
    "minibatch_size_values = [32, 64]\n",
    "memory_max_size_values = [10000]\n",
    "initial_epsilon_values = [1.0]\n",
    "epsilon_decay_values = [0.996, 0.99]\n",
    "gamma_values = [0.9, 0.95]\n",
    "learning_rate_values = [0.001, 0.0001]\n",
    "\n",
    "best_reward = float('-inf')\n",
    "best_hyperparameters = None\n",
    "\n",
    "# Perform the grid search\n",
    "for num_episodes in num_episodes_values:\n",
    "    for minibatch_size in minibatch_size_values:\n",
    "        for memory_max_size in memory_max_size_values:\n",
    "            for initial_epsilon in initial_epsilon_values:\n",
    "                for epsilon_decay in epsilon_decay_values:\n",
    "                    for gamma in gamma_values:\n",
    "                        for learning_rate in learning_rate_values:\n",
    "                            print(f\"Training with hyperparameters - num_episodes: {num_episodes}, minibatch_size: {minibatch_size}, memory_max_size: {memory_max_size}, initial_epsilon: {initial_epsilon}, epsilon_decay: {epsilon_decay}, gamma: {gamma}, learning_rate: {learning_rate}\")\n",
    "                            reward = train_and_evaluate(num_episodes, minibatch_size, memory_max_size, initial_epsilon, epsilon_decay, gamma, learning_rate)\n",
    "                            print(f\"Hyperparameters eval: {num_episodes, minibatch_size, memory_max_size, initial_epsilon, epsilon_decay, gamma, learning_rate}, reward: {reward}\")\n",
    "                            if reward > best_reward:\n",
    "                                best_reward = reward\n",
    "                                best_hyperparameters = (num_episodes, minibatch_size, memory_max_size, initial_epsilon, epsilon_decay, gamma, learning_rate)\n",
    "\n",
    "print(f\"Best hyperparameters: {best_hyperparameters}, Best reward: {best_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a4bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Hyperparameters\n",
    "num_episodes = 10000\n",
    "minibatch_size = 64\n",
    "memory_max_size = 100000\n",
    "initial_epsilon = 1.0\n",
    "min_epsilon = 0.1\n",
    "epsilon_decay = 0.996\n",
    "gamma = 0.9\n",
    "\n",
    "reward_sums = []\n",
    "replay_memory = []\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    surgical_environment.reset_environment()\n",
    "    done = False\n",
    "    reward_sum = 0\n",
    "    \n",
    "    while not done:\n",
    "        state = preprocess_state(surgical_environment.surgical_grid)\n",
    "        \n",
    "        if np.random.rand() < initial_epsilon:\n",
    "            action = random.choice(surgical_environment.action_space)\n",
    "        else:\n",
    "            q_values = model.predict(state)\n",
    "            action = np.argmax(q_values)\n",
    "        \n",
    "        next_state, reward, done, _ = surgical_environment.perform_action(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        replay_memory.append({'s': state, 'a': action, 'r': reward, 's_prime': preprocess_state(next_state), 'done': done})\n",
    "        \n",
    "        if len(replay_memory) > memory_max_size:\n",
    "            replay_memory.pop(0)\n",
    "        \n",
    "        if len(replay_memory) >= minibatch_size:\n",
    "            minibatch = random.sample(replay_memory, minibatch_size)\n",
    "            inputs = np.zeros((minibatch_size, *input_shape))\n",
    "            targets = np.zeros((minibatch_size, num_actions))\n",
    "            \n",
    "            for i, sample in enumerate(minibatch):\n",
    "                s, a, r, sprime, d = sample['s'], sample['a'], sample['r'], sample['s_prime'], sample['done']\n",
    "                inputs[i] = s\n",
    "                targets[i] = model.predict(s)\n",
    "                if not d:\n",
    "                    targets[i, a] = r + gamma * np.max(model.predict(sprime))\n",
    "                else:\n",
    "                    targets[i, a] = r\n",
    "            \n",
    "            model.fit(inputs, targets, epochs=1, verbose=0)\n",
    "\n",
    "    initial_epsilon *= epsilon_decay\n",
    "    initial_epsilon = max(initial_epsilon, min_epsilon)\n",
    "    reward_sums.append(reward_sum)\n",
    "    plt.plot(reward_sums, color='blue', linewidth=1)  # plot of the rewards\n",
    "    if episode >= 3:  # after 3 episodes, plot the trend line\n",
    "        x = np.linspace(0, len(reward_sums), len(reward_sums), dtype=np.int32)\n",
    "        z = np.polyfit(x, reward_sums, 3)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(x, p(x), linewidth=2, color='orange')\n",
    "\n",
    "    plt.title(f'Episode {episode+1},  \\u03B5= {round(initial_epsilon, 3)},  Avg reward={round(mean(reward_sums), 3)}')\n",
    "\n",
    "    if episode != num_episodes - 1:  # until the training is not finished\n",
    "        plt.draw()\n",
    "        plt.pause(1.0)\n",
    "        plt.clf()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}/{num_episodes}, Total Reward: {reward_sum}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('./agent' + str(num_episodes) + '.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95585551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "trained_agent = load_model('./agent10000.h5')  # loading the trained agent model\n",
    "\n",
    "test_environment = SurgicalEnvironment()\n",
    "test_environment.reset_environment()\n",
    "frames = [test_environment.surgical_grid.copy()]  # create frames for animation\n",
    "finished = False\n",
    "total_reward = 0\n",
    "\n",
    "\n",
    "while not finished:\n",
    "    state = preprocess_state(test_environment.surgical_grid)\n",
    "    q_values = trained_agent.predict(state)\n",
    "    action = np.argmax(q_values)\n",
    "    _, reward, finished, info = test_environment.perform_action(action)\n",
    "    print('Path:', info['path_history'])\n",
    "    print('Actions:', info['actions_taken'])\n",
    "    print()\n",
    "    total_reward += reward\n",
    "\n",
    "    frames.append(test_environment.surgical_grid.copy())  # store the frame\n",
    "\n",
    "print('Total reward:', total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299497a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "\n",
    "fps = len(frames)\n",
    "n_seconds = len(frames)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "a = frames[0]\n",
    "im = plt.imshow(a, interpolation='none', aspect='auto', vmin=0, vmax=255)\n",
    "anim = FuncAnimation(\n",
    "    fig,\n",
    "    animate_frame,\n",
    "    frames=len(frames),\n",
    "    interval=10000 / fps,  # ms\n",
    ")\n",
    "\n",
    "anim.save('./agent_' + str(num_episodes) + '.gif', writer='imagemagick', fps=int(fps / 2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
